{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNXUEQ0m7U78td6s02tVvQ5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0I_sR72Ma0s"
      },
      "source": [
        "# how batch normalization can fix problems in training\n",
        "# Won't do any standardization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT3SxUxaMa3T"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "concrete = pd.read_csv('../datasets/concrete.csv')\n",
        "df = concrete.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF0CE_fhMa6E"
      },
      "source": [
        "df_train = df.sample(frac=0.7, random_state=0)\n",
        "df_valid = df.drop(df_train.index)\n",
        "\n",
        "X_train = df_train.drop('CompressiveStrength', axis=1)\n",
        "X_valid = df_valid.drop('CompressiveStrength', axis=1)\n",
        "y_train = df_train['CompressiveStrength']\n",
        "y_valid = df_valid['CompressiveStrength']\n",
        "\n",
        "input_shape = [X_train.shape[1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYeQCnHeMa8v"
      },
      "source": [
        "# train the network on the unstandardized data\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation='relu', input_shape=input_shape),\n",
        "    layers.Dense(512, activation='relu'),    \n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='sgd', # SGD is more sensitive to differences of scale\n",
        "    loss='mae',\n",
        "    metrics=['mae'],\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=64,\n",
        "    epochs=100,\n",
        "    verbose=0,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNWDyLKiMa_e"
      },
      "source": [
        "# Training ends up with a blank graph\n",
        "\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.loc[0:, ['loss', 'val_loss']].plot()\n",
        "print((\"Minimum Validation Loss: {:0.4f}\").format(history_df['val_loss'].min()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCuFiWrZNTf6"
      },
      "source": [
        "Trying to train this network on this dataset will usually fail. \n",
        "\n",
        "Even when it does converge (due to a lucky weight initialization), it tends to converge to a very large number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0HHmPtaMbCQ"
      },
      "source": [
        "# Add a BatchNormalization layer before each Dense layer\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(512, activation='relu', input_shape=input_shape),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1),\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPccmjISMbFA"
      },
      "source": [
        "model.compile(\n",
        "    optimizer='sgd',\n",
        "    loss='mae',\n",
        "    metrics=['mae'],\n",
        ")\n",
        "\n",
        "EPOCHS = 100\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=64,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=0,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xed_AfhaMbH0"
      },
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.loc[0:, ['loss', 'val_loss']].plot()\n",
        "print((\"Minimum Validation Loss: {:0.4f}\").format(history_df['val_loss'].min()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAIiGxHfNw-k"
      },
      "source": [
        "Adding batch normalization was a big improvement on the first attempt \n",
        "\n",
        "By adaptively scaling the data as it passes through the network, batch normalization can let us train models on difficult datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSpBrzgEMbKw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}